{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So my own SGD doesn't seem to be converging towards what the analytical solution gives. The squared loss function is convex due to X.TX always being positive and semi definite, so it should with enough epochs converge towards analytical. Sklearn is able to reach it with a lot of iterations. Though they might be using some fancy stuff for quicker convergence. Might test with momentum too though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "sys.path.append('../src/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from modelling import ols,ridge\n",
    "from modelling.sgd import SGD_optimizer\n",
    "from data.create_dataset import *\n",
    "from visualization.visualize import *\n",
    "from sklearn.model_selection import  train_test_split\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, z = create_dataset('../data/raw/SRTM_data_Norway_1.tif',degree=4)\n",
    "X_train, X_test, z_train, z_test = train_test_split(X,z, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1161.60416278],\n",
       "       [  731.60705663],\n",
       "       [  406.09728099],\n",
       "       [-1494.27341142],\n",
       "       [-2223.48693796],\n",
       "       [ -788.65010401],\n",
       "       [ 1696.80944233],\n",
       "       [ 3307.8261914 ],\n",
       "       [  817.63621267],\n",
       "       [ 1138.02663525],\n",
       "       [ -894.20916574],\n",
       "       [ -717.52395709],\n",
       "       [-1619.21466643],\n",
       "       [  755.01568821],\n",
       "       [ -795.77629324]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beta_ols = ols.fit_beta(X_train,z_train)\n",
    "beta_ols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above is truth or analytical solution. SGD should reach with enough epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xscaler = StandardScaler().fit(X_train)\n",
    "zscaler = StandardScaler().fit(z_train)\n",
    "\n",
    "#X_train_scl = Xscaler.transform(X_train)\n",
    "#z_train_scl = zscaler.transform(z_train)\n",
    "X_train_scl = X_train - np.mean(X_train,axis=0)\n",
    "z_train_scl = z_train - np.mean(z_train,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[    0.           677.88172141   366.72735468 -1325.92684199\n",
      " -2122.07632564  -678.30074455  1486.13118085  3183.40664783\n",
      "   713.47476958  1007.59515263  -804.24161291  -664.53285218\n",
      " -1576.6274202    794.63845807  -741.94133067]\n",
      "[0.]\n"
     ]
    }
   ],
   "source": [
    "sgdreg = SGDRegressor( random_state=10, fit_intercept = False, max_iter = 100000, penalty=None,alpha=1,learning_rate = 'constant', eta0 = 0.001,tol=None)\n",
    "sgdreg = sgdreg.fit(X_train_scl,z_train_scl.ravel())\n",
    "print(sgdreg.coef_.T)\n",
    "print(sgdreg.intercept_.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  678.20515241   367.02555952 -1326.31244476 -2122.43873302\n",
      "  -678.64138267  1486.78080198  3183.88749225   713.99335689\n",
      "  1008.1678578   -804.41847264  -664.70927845 -1576.76259406\n",
      "   794.45534669  -742.06405811]\n",
      "[1167.7579321]\n"
     ]
    }
   ],
   "source": [
    "sgdreg2 = SGDRegressor( random_state=10, fit_intercept = True, max_iter = 100000, penalty=None,alpha=1,learning_rate = 'constant', eta0 = 0.001,tol=None)\n",
    "sgdreg2 = sgdreg2.fit(X_train[:,1:],z_train.ravel())\n",
    "print(sgdreg2.coef_.T)\n",
    "print(sgdreg2.intercept_.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So after having gone through whole SKlearn SGD source code I've figured out that they actually calculate intercept and weights separately. It seems they initially do the X@beta - z , add this to the intercept, then completes the 1/n(X.T(X@beta-z)). However I can't see that they actually multiply by two for some reason. Even though the gradient should have a 2 in front. But updating intercept afterwards is actually exactly the same as only keeping it in the design matrix. Because they updates the weight with $X\\times c$ where c is prediction-truth. This same scalar is directly added to the intercept, which is the same as first scaling it with 1, which is in the first column of X, and then add it. Simply keeping 1's i n the design matrix should result in the same.\n",
    "\n",
    "What needs to happen with Ridge, is that first column must be pulled out, data must be centered, and then during cost gradient, pull out X@beta -z and return as well, or do similar as sklearn, simply do prediction and subtract z in a function, which we call update or something, continue running this one further for rest of betas, but add to intercept. So I need prediction at some point. But sum of all? Because I'm operating with batches, and not one sample at a time like SKlearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8000, 1)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(X_train.T @ (X_train @ beta_ols-z_train)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test\n",
      "8000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[   -0.204324  ],\n",
       "       [  677.84250857],\n",
       "       [  366.21086142],\n",
       "       [-1326.4926003 ],\n",
       "       [-2121.39395492],\n",
       "       [ -678.33446858],\n",
       "       [ 1487.04854988],\n",
       "       [ 3183.95371078],\n",
       "       [  712.98282652],\n",
       "       [ 1007.29483881],\n",
       "       [ -804.14021266],\n",
       "       [ -664.86039468],\n",
       "       [-1576.16098222],\n",
       "       [  795.11487984],\n",
       "       [ -742.5917441 ]])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sgdown = ols.fit_beta_sgd(X_train_scl,z_train_scl,batch_size=1,n_epochs=100000, lr=0.001)\n",
    "sgdown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok good, so it does converge correctly, with same epochs and lr as sklearn. Only problem is the intercept now. How to calculate? I simply must rescale afterwards. So if I'd want to get the same intercept as SKlearn?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<modelling.sgd.SGD_optimizer at 0x24aaced43d0>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sgd = SGD_optimizer(batch_size = 1, n_epochs = 1,use_momentum= False, gamma = 0.5,regularization = None, lmb = 1, lr=0.001)\n",
    "beta_class = sgd.fit(X_train_scl,z_train_scl)\n",
    "beta_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ],\n",
       "       [36.44979845],\n",
       "       [-4.72354289],\n",
       "       [34.90751015],\n",
       "       [28.55293119],\n",
       "       [-6.09129011],\n",
       "       [29.05666631],\n",
       "       [30.78957577],\n",
       "       [23.05939687],\n",
       "       [-6.70535627],\n",
       "       [23.46525216],\n",
       "       [27.97684843],\n",
       "       [25.38675151],\n",
       "       [19.30913268],\n",
       "       [-7.06176838]])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beta_class.beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
