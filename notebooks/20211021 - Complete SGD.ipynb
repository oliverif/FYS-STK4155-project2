{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69b97041",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "sys.path.append('../src/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5caa6373",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.create_dataset import *\n",
    "from visualization.visualize import *\n",
    "from modelling import ols,ridge\n",
    "from model_evaluation.metrics import *\n",
    "from processing.data_preprocessing import *\n",
    "from utils.utils import *\n",
    "import numpy as np\n",
    "from sklearn.model_selection import  train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d3a385b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9fc9623",
   "metadata": {},
   "outputs": [],
   "source": [
    "beta = np.random.randn(2,1)\n",
    "n = 1000\n",
    "x = 2*np.random.rand(n,1)\n",
    "y = 4+3*x+np.random.randn(n,1)\n",
    "\n",
    "X = np.c_[np.ones((n,1)), x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9d461d8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.82630258],\n",
       "       [3.09085066]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beta_real = ols.fit_beta(X,y)\n",
    "beta_real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e8ddf6bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "sgdreg = SGDRegressor(max_iter = 100, penalty=None, eta0=0.01, tol=None)\n",
    "sgdreg = sgdreg.fit(x,y.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "db3e44c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([3.82248455]), array([3.08709076])]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[sgdreg.intercept_,sgdreg.coef_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "24dc68d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.82203767],\n",
       "       [3.09061134]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beta = ols.fit_beta_sgd(X,y,0.01,100,100)\n",
    "beta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "655767ce",
   "metadata": {},
   "source": [
    "So when making a schedule for lr for instance using \n",
    "$\n",
    "\\gamma = \\frac{t_0}{em+i + t_1}\n",
    "$\n",
    "it seems that the lower the t1 the faster the lr will decrease in the beginning epochs. t0 seems to mostly be affectly the interval at which the decrease happens. Lower t0 means that the lr decreases faster in the beginning and less later. t1 simple shifts the curve horizontally, which is why it can cause faster decrease in the beginning. I.e the vertical asymptote is at -t1.\n",
    "\n",
    "So why use learning_schedule? Seems like it's mostly so that the optimization converges faster. This is because it allows you to have larger learning rates in the beginning and small enough at the end. Notice 'small enough', recall that learning rate needs to be sufficiently small to ensure convergence. However, should it not be able to converge anyways? If it has enough n_epochs? with learning rate 1 and 10000 epochs it's closing in on the real betas. Though not very well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "48fc1922",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.06058942],\n",
       "       [1.87303311]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beta = ols.fit_beta_sgd(X,y,1,100,10000)\n",
    "beta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e4e29e1",
   "metadata": {},
   "source": [
    "How to implement? Say we use above function, we need then to define t0 and t1, should make class out of it? Maybe a bit too much? Could simply decide something, don't want another parameter to tweak, since it will only alter the training speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f4217cce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.84556117],\n",
       "       [3.07184904]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beta = ols.fit_beta_sgd(X,y,0.1,100,100)\n",
    "beta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5764581",
   "metadata": {},
   "source": [
    "Maybe make it optional with learning schedule? In that case how?\n",
    "Because I want to pass one argument either as a number indicating constant or a schedule which decides learning rate based on epoch and batch. Thing is, I don't want an if statement in the loop, which is slow, I want the function return the same lr if set to constant, but return a variable one if set to schedule. Which means, that in one case it only needs lr as input, and then return it again. And the other case it needs epoch, batch and num batches as input. So either way, must be able to vary what input is given. But that I don't think can be done, unless passing a list? That might work, might be ugly though. Why even list? That would require me to input every parameter to both functions, but use some of them based on which function. Can still do this..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "5c1e4dd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.84313045],\n",
       "       [3.07424732]])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beta = ols.fit_beta_sgd(X,y, batch_size=100,n_epochs=100)\n",
    "beta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8882db",
   "metadata": {},
   "source": [
    "Now we do sgd on ridge. I guess it's the cost function that's the only difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "d306437c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.20757294],\n",
       "       [1.72801081]])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beta_ridge = ridge.fit_beta_sgd(X,y, batch_size=100,n_epochs=10,lr=0.01)\n",
    "beta_ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "df390448",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([3.8238777]), array([3.08625028])]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "sgdreg = SGDRegressor(max_iter = 100, penalty='l2', eta0=0.01, tol=None)\n",
    "sgdreg = sgdreg.fit(x,y.ravel())\n",
    "[sgdreg.intercept_,sgdreg.coef_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ddc963",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
