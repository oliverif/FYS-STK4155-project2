{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part b and c\n",
    "This notebook contains a step-by-step walkthrough of task b) and c) using both code and text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "#Add own modules to path\n",
    "import sys\n",
    "sys.path.append('../..')\n",
    "sys.path.append('../../src/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the data and constructing the input matrix\n",
    "The same data as from previous exercises is reused. However, as a neural network will be fitted in this case, the design matrix is not created in the same manner.\n",
    "\n",
    "When a neural network trains on terrain data, it takes in the x and y coordinates and tries to predict the output z. As such, the design matrix should be of shape (n_samples,2). In other words, extract the data without creating a design matrix, or simply use the two columns from the original design matrix X[:,1:3].\n",
    "\n",
    "Note that both the target data and design matrix is scaled follow the same arguments as in part a. Scaling the target data might be more beneficial with neural networks as it can help avoid exploding gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'src'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1144/2321835205.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_dataset\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcreate_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m  \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvisualization\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvisualize\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mplot_surf_from_X\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'src'"
     ]
    }
   ],
   "source": [
    "from src.data.create_dataset import create_dataset\n",
    "from sklearn.model_selection import  train_test_split\n",
    "from src.visualization.visualize import plot_surf_from_X\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X, z = create_dataset('../../data/raw/SRTM_data_Norway_1.tif')\n",
    "X_train, X_test, z_train, z_test = train_test_split(X,z, test_size=0.2)\n",
    "\n",
    "X_scl = StandardScaler().fit(X_train)\n",
    "z_scl = StandardScaler().fit(z_train)\n",
    "\n",
    "X_train = X_scl.transform(X_train)\n",
    "X_test = X_scl.transform(X_test)\n",
    "z_train = z_scl.transform(z_train)\n",
    "z_test = z_scl.transform(z_test)\n",
    "\n",
    "plot_surf_from_X(X,z,'All data')\n",
    "plot_surf_from_X(X_train,z_train,'Train data')\n",
    "plot_surf_from_X(X_test,z_test,'Test data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression using Neural Network\n",
    "In this part a neural network is fitted to the terrain data. In other words, it is used as regression as opposed to classification which is perhaps the more familiar.\n",
    "\n",
    "#### Neural network in short\n",
    " A neural network consists of layers of nodes fully connected to all nodes in the layers at either side. These weighted connections goes through an activation function causing the non-linearities in the model. A node's state is the weighted sum av all activations coming in to it. Additionally there is an extra bias added to each node as a sort of \"intercept\".\n",
    "\n",
    " These weights and biases is fitted to a data set, meaning they are tuned so that when predicting output, a cost function is minimized. SGD is used very similarly as with the linear regression case, where each parameter is subtracted the gradient. In the case of neural network the gradient needs to be calculated for every connection, meaning a lot more gradients than with linear regression.\n",
    "\n",
    " Prediction is perform using the algorithm feedforward, which feeds the input value forward through the connections and activation functions. The update of the parameters is then done using the backpropagation algorithm, which starts by calculating the error and gradient of the last layer, and uses that result to calculate the gradient and error in the previous layer, and so forth.\n",
    "\n",
    " As with linear regression, momentum, regularization and different learning rate schemes may be employed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.modelling.nn import NeuralNetwork\n",
    "from src.modelling.linreg import LinReg\n",
    "from src.model_evaluation.metrics import MSE_R2\n",
    "import numpy as np\n",
    "\n",
    "#Model parameters\n",
    "batch_size = 64\n",
    "n_epochs = 1000\n",
    "lr0 = 0.01\n",
    "loss_function = 'squared_loss'\n",
    "hidden_layers = (50,)\n",
    "hidden_activation = 'sigmoid'\n",
    "output_activation = 'linear'\n",
    "lmb = 0.01\n",
    "lr0 = 0.01\n",
    "\n",
    "#Fitting neural network\n",
    "nn = NeuralNetwork(batch_size = batch_size,\n",
    "                   n_epochs = n_epochs,\n",
    "                   hidden_layers = hidden_layers, \n",
    "                   w_init = 'normal',\n",
    "                   loss_func = loss_function,\n",
    "                   val_fraction=0,\n",
    "                   hidden_activation=hidden_activation,\n",
    "                   output_activation=output_activation,\n",
    "                   lmb = lmb,\n",
    "                   lr0= lr0)\n",
    "\n",
    "nn.fit(X_train,z_train)\n",
    "tilde_nn = nn.predict(X_train)\n",
    "pred_nn = nn.predict(X_test)\n",
    "\n",
    "#Fitting ols for comparison\n",
    "ols =LinReg(regularization = None).fit(X_train,z_train)\n",
    "tilde_ols = ols.predict(X_train)\n",
    "pred_ols = ols.predict(X_test)\n",
    "\n",
    "train_mse_nn, train_r2_nn = MSE_R2(z_train,tilde_nn)\n",
    "test_mse_nn, test_r2_nn = MSE_R2(z_test ,pred_nn)\n",
    "train_mse_ols, train_r2_ols = MSE_R2(z_train,tilde_ols)\n",
    "test_mse_ols, test_r2_ols = MSE_R2(z_test ,pred_ols)\n",
    "\n",
    "#Printing the scores\n",
    "print('NN train[MSE,R2]:',(train_mse_nn, train_r2_nn), '\\nNN test[MSE,R2]:',(test_mse_nn, test_r2_nn))\n",
    "print('OLS train[MSE,R2]:',(train_mse_ols, train_r2_ols), '\\nOLS test[MSE,R2]:',(test_mse_ols, test_r2_ols))\n",
    "\n",
    "#Concatenating X_train and X_test so that we \n",
    "#can plot the whole predicted surface\n",
    "X_ = np.concatenate((X_train,X_test))\n",
    "\n",
    "plot_surf_from_X(X_,np.concatenate((tilde_ols,pred_ols)),'OLS')\n",
    "plot_surf_from_X(X_,np.concatenate((tilde_nn,pred_nn)),'Neural Network')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hope result is bad due to w_init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "#Model parameters\n",
    "batch_size = 64\n",
    "n_epochs = 1000\n",
    "lr0 = 0.01\n",
    "loss_function = 'squared_loss'\n",
    "hidden_layers = (50,)\n",
    "lmb = 0.01\n",
    "lr0 = 0.01\n",
    "\n",
    "#Setting new parameters\n",
    "nn.set_params(batch_size = batch_size,\n",
    "              n_epochs = n_epochs,\n",
    "              hidden_layers = hidden_layers, \n",
    "              w_init = 'glorot',\n",
    "              loss_func = loss_function,\n",
    "              val_fraction=0,\n",
    "              hidden_activation=hidden_activation,\n",
    "              output_activation=output_activation,\n",
    "              lmb = lmb,\n",
    "              lr0= lr0)\n",
    "#Fitting network\n",
    "nn.fit(X_train,z_train)\n",
    "\n",
    "nn_sk = MLPRegressor(batch_size = batch_size,\n",
    "                     max_iter = n_epochs,\n",
    "                     hidden_layer_sizes = hidden_layers, \n",
    "                     activation='logistic',\n",
    "                     solver = 'sgd',\n",
    "                     batch_size=batch_size,\n",
    "                     learning_rate_init=lr0,\n",
    "                     tol = 0,\n",
    "                     momentum = 0.5,\n",
    "                     validation_fraction = 0)\n",
    "nn_sk.fit(X_train,z_train)\n",
    "\n",
    "#Predicting\n",
    "tilde_nn = nn.predict(X_train)\n",
    "pred_nn = nn.predict(X_test)\n",
    "tilde_sk = nn_sk.predict(X_train)\n",
    "pred_sk = nn_sk.predict(X_test)\n",
    "#Calcluating MSE and R2\n",
    "train_mse_nn, train_r2_nn = MSE_R2(z_train,tilde_nn)\n",
    "test_mse_nn, test_r2_nn = MSE_R2(z_test ,pred_nn)\n",
    "train_mse_sk, train_r2_sk = MSE_R2(z_train,tilde_sk)\n",
    "test_mse_sk, test_r2_sk = MSE_R2(z_test ,pred_sk)\n",
    "\n",
    "#Printing the scores\n",
    "print('NN train[MSE,R2]:',(train_mse_nn, train_r2_nn), '\\nNN test[MSE,R2]:',(test_mse_nn, test_r2_nn))\n",
    "print('NN train[MSE,R2]:',(train_mse_sk, train_r2_sk), '\\nNN test[MSE,R2]:',(test_mse_sk, test_r2_sk))\n",
    "print('OLS train[MSE,R2]:',(train_mse_ols, train_r2_ols), '\\nOLS test[MSE,R2]:',(test_mse_ols, test_r2_ols))\n",
    "#Plotting the predicted surfaces\n",
    "plot_surf_from_X(X_,np.concatenate((tilde_ols,pred_ols)),'OLS')\n",
    "plot_surf_from_X(X_,np.concatenate((tilde_nn,pred_nn)),'Neural Network')\n",
    "plot_surf_from_X(X_,np.concatenate((tilde_sk,pred_sk)),'Sklearn MLPRegressor')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much better with glorot because and probably equal to sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning parameters\n",
    "#### Gridsearching learning rate and lambda\n",
    "Similarily as with SGD regression performing a grid search on the lr and lmb is a common first step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.model_evaluation.param_analysis import grid_search_df\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "472da4574dc3dc7c8d87a9dc22810c195d38b2ca3ea0ccf42e41b431859154af"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
